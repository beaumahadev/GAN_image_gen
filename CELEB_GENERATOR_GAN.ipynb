{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJQN3W3ec2pNi47luYcJqB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beaumahadev/GAN_image_gen/blob/main/CELEB_GENERATOR_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TSyHcH3c7ft3"
      },
      "outputs": [],
      "source": [
        "# Advanced GAN\n",
        "\n",
        "\n",
        "import torch, torchvision, os, PIL, pdb\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#show the grid of images\n",
        "def show(tensor, num= 25, name=''):\n",
        "  data = tensor.detach().cpu()\n",
        "  grid = make_grid(data[:num], nrow=5).permute(1,2,0)\n",
        "  plt.imshow(grid.clip(0,1))\n",
        "  plt.show()\n",
        "\n",
        "### hyperparameters and general parameters\n",
        "n_epochs=10000\n",
        "batch_size=128\n",
        "lr=1e-4\n",
        "z_dim=200\n",
        "device='cuda' #GPU\n",
        "cur_step=0\n",
        "#5 cycles of training for the crit every one cycle for the generator, because otherwise the gen might overpower the critic (fool the critic too early)\n",
        "crit_cycles=5\n",
        "gen_losses=[]\n",
        "crit_losses=[]\n",
        "show_step=35\n",
        "save_step=35\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generator model\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, z_dim=64, d_dim=16):\n",
        "    super(Generator, self).__init__()\n",
        "    self.z_dim = z_dim\n",
        "\n",
        "    self.gen = nn.Sequential(\n",
        "        ## ConvTranspose2d: in_channels, out_channels, kernel_size, stride=1, padding=0\n",
        "        ## Calulating new width and height: (n-1)*stride - 2*padding + ks\n",
        "        ## input channels in the dimensionality of the latent space\n",
        "        ## start with z_dim # of channels and 1 pixel image, decrease channels and increase size\n",
        "        nn.ConvTranspose2d(z_dim, d_dim *32, 4, 1, 0), # 4x4 0-0+4 (ch: 200, 512)\n",
        "        nn.BatchNorm2d(d_dim * 32),\n",
        "        nn.Relu(True),\n",
        "\n",
        "        nn.ConvTranspose2d(z_dim*32, d_dim*16, 4, 2, 1), ## 8x8 (ch: 512, 256)\n",
        "        nn.BatchNorm2d(d_dim*16),\n",
        "        nn.Relu(True),\n",
        "\n",
        "        nn.ConvTranspose2d(d_dim*16, d_dim*8, 4, 2, 1), ## 16x16 (ch: 256, 128)\n",
        "        nn.BatchNorm2d(d_dim*8),\n",
        "        nn.Relu(True),\n",
        "\n",
        "        nn.ConvTranspose2d(d_dim*8, d_dim*4, 4, 2, 1), ## 32x32 (ch: 128, 64)\n",
        "        nn.BatchNorm2d(d_dim*4),\n",
        "        nn.Relu(True),\n",
        "\n",
        "        nn.ConvTranspose2d(d_dim*4, d_dim*2, 4, 2, 1), ## 64x64 (ch: 64, 32)\n",
        "        nn.BatchNorm2d(d_dim*2),\n",
        "        nn.Relu(True),\n",
        "\n",
        "        nn.ConvTranspose2d(d_dim*2, 3, 4, 2, 1), ## 128x128 (ch: 32, 3)\n",
        "        nn.Tanh() ### produce a result in the range from -1, 1\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, noise):\n",
        "    x= noise.view(len(noise), self.z_dim, 1, 1) # 128 x 200 x 1 x 1\n",
        "    return self.gen(x)\n",
        "\n",
        "def gen_noise(num, z_dim, device=\"cuda\"):\n",
        "  return torch.random(num, z_dim, device=device) # 128 x 200 noise vector (dimensionality of the latent space)\n"
      ],
      "metadata": {
        "id": "9suHiM7STHbA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## critic model\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self, d_dim=16):\n",
        "    super(Critic, self).__init__()\n",
        "\n",
        "    self.crit  = nn.Sequential(\n",
        "        # Conv2d: in_channels, out_channels, kernel_size, stride=1 padding=0\n",
        "        ## New width and height: ((n+2*pad-ks)//stride) +1\n",
        "        nn.Conv2d(3, d_dim, 4, 2, 1), #  (128+2*(1-4)//2)+1 = 64\n",
        "        nn.InstanceNorm2d(d_dim), #Normalization is a good way to stabilize numbers between layers of the neural net\n",
        "        ## We can normalize by batch, channel, or instance. For critic instance works best\n",
        "        nn.LeakyReLU(0.2), ## Avoid dying Relu\n",
        "\n",
        "        nn.Conv2d(d_dim, d_dim*2, 4, 2, 1),\n",
        "        nn.InstanceNorm2d(d_dim*2),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(d_dim*2, d_dim*4, 4, 2, 1), ## 16x16 (ch:32->64)\n",
        "        nn.InstanceNorm2d(d_dim*4),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(d_dim*4, d_dim*8, 4, 2, 1), ## 8x8 (ch:64->128)\n",
        "        nn.InstanceNorm2d(d_dim*8),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(d_dim*8, d_dim*16, 4, 2, 1), ## 4x4 (ch:128->256)\n",
        "        nn.InstanceNorm2d(d_dim*16),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(d_dim*16, 1, 4, 1, 0), #do this to get the right final size\n",
        "        # final size: (4+2*0-4)//1 +1 = 1x1\n",
        "        # channels: 256, 1\n",
        "    )\n",
        "\n",
        "  def forward(self, image):\n",
        "    # image: 128 batch x 3 channels x 128 w x 128 h\n",
        "    crit_pred = self.crit(image) # 128 batch, 1 channel, 1 w, 1 h (128 values for each image in the batch\n",
        "    return crit_pred.view(len(crit_pred), -1)\n"
      ],
      "metadata": {
        "id": "DjzdYW3xWIDc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example of overwriting pytorch initial weights\n",
        "\n",
        "def __init_weights(m):\n",
        "  if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "    torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "  if isinstance(m, nn.BatchNorm2d):\n",
        "    torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "# example gen = gen.apply(init_weights)"
      ],
      "metadata": {
        "id": "uC8o5b9PVK3O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Training dataset download address:\n",
        "# Celebra gdrive: https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg?resourcekey=0-rJlzl934LzC-Xp28GeIBzQ\n",
        "# Kaggle: https://www.kaggle.com/jessicali9530/celeba-dataset"
      ],
      "metadata": {
        "id": "z0P3ofto1zOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Dataset, DataLoader, declare gen,crit, test dataset\n",
        "\n",
        "class Dataset(Dataset):\n",
        "  def __init__(self, path, size=128, lim=10000):\n",
        "    self.sizes=[size, size]\n",
        "    items, labels=[],[]\n",
        "\n",
        "    for data in os.listdir(path)[:lim]:\n",
        "      #path: './data/celeba/img_align_celeba'\n",
        "      #data: '114568.jpg\n",
        "      item = os.path.join(path,data)\n",
        "      items.append(item)\n",
        "      labels.append(data)\n",
        "    self.items=items\n",
        "    self.labels=labels\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.items)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    data = PIL.Image.open(self.items[idx]).convert('RGB') # (178,218)\n",
        "    data = np.asarray(torchvision.transforms.Resize(self.sizes)(data)) # 128 x 128 x 3\n",
        "    data = np.transpose(data, (2,0,1)).astype(np.float32, copy=False) # 3 x 128 x 128 # from 0 to 255\n",
        "    data = torch.from_numpy(data).div(255) # from 0 to 1\n",
        "    return data, self.labels[idx]\n",
        "\n",
        "## Dataset\n",
        "data_path='./data/celeba/img_align_celeba'\n",
        "ds = Dataset(data_path, size=128, lim=10000)\n",
        "\n",
        "## DataLoader\n",
        "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "## Models\n",
        "gen = Generator(z_dim).to(device)\n",
        "crit = Critic().to(device)\n",
        "\n",
        "## Optimizers\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(0.5,0.9))\n",
        "crit_opt= torch.optim.Adam(crit.parameters(), lr=lr, betas=(0.5,0.9))\n",
        "\n",
        "## Initializations\n",
        "##gen=gen.apply(init_weights)\n",
        "##crit=crit.apply(init_weights)\n",
        "\n",
        "x,y=next(iter(dataloader))\n",
        "show(x)\n"
      ],
      "metadata": {
        "id": "KKk99vNTBULm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}